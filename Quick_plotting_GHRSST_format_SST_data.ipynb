{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/tools/frameworks/-/raw/main/img/Standard_banner.png' align='right' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Marine Training Service**</font> <br>\n",
    "**Copyright:** 2023 EUMETSAT <br>\n",
    "**License:** MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>GHRSST24 2023</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>PREREQUISITES </b>\n",
    "    \n",
    "This notebook has the following prerequisites:\n",
    "- **<a href=\"https://data.marine.copernicus.eu/register\" target=\"_blank\">A Copernicus Marine Service (CMEMS) account</a>** to enable you to download the data needed for this notebook.\n",
    "\n",
    "There are no other prerequisite notebooks for this module, but you may wish to consider the following related notebooks:\n",
    "\n",
    "- **<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies/-/blob/main/Case_studies/Ocean_phenomena/Basin_scale_variability/Atlantic_Med_SST_anomalies_MHW_2023/Atlantic_Med_SST_anomalies_2023.ipynb\" target=\"_blank\">Sea surface temperature anomalies in the Northern Atlantic and Mediterranean Sea in 2023\n",
    "</a>**\n",
    "- **<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies/-/blob/main/Case_studies/Ocean_phenomena/Basin_scale_variability/Atlantic_Med_SST_anomalies_MHW_2023/Med_MHW_2023.ipynb\" target=\"_blank\">Marine heatwaves in the Mediterranean Sea in 2023\n",
    "</a>**\n",
    "- **<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/sensors/learn-slstr\" target=\"_blank\">Learn SLSTR</a>**\n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick plotting GHRSST-format SST data\n",
    "\n",
    "### Data used\n",
    "\n",
    "| Product Description | WEkEO HDA ID | WEkEO metadata |\n",
    "|:--------------------:|:-----------------:|:-----------------:|\n",
    "| Global OSTIA SST (Reprocessed) | EO:MO:DAT:SST_GLO_SST_L4_REP_OBSERVATIONS_010_011 | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AMO%3ADAT%3ASST_GLO_SST_L4_REP_OBSERVATIONS_010_011\" target=\"_blank\">link</a> |\n",
    "| Global OSTIA SST (Near real-time) | EO:MO:DAT:SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001 | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AMO%3ADAT%3ASST_GLO_SST_L4_NRT_OBSERVATIONS_010_001\" target=\"_blank\">link</a> |\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know;\n",
    "* how to download data from the Copernicus Marine Service Data Store using OpenDap\n",
    "* how to make a variety of simple plots with sea surface tmperature data\n",
    "  \n",
    "### Outline\n",
    "\n",
    "Sea surface temperature (SST) is an essential climate variable and measuring it is key to understanding ocean dynamics. Satellite SST products are typically distributed as spatial fields, either on an instrument grid (for Level-1B and Level-2 products) or projected grid (for Level-3 and Level-4 products). This short notebook shows how to use Python to acquire Level-4 GHRSST format SST data form the <a href=\"https://marine.copernicus.eu/\" target=\"_blank\">Copernicus Marine Service</a> and plot it using the <a href=\"http://bokeh.org/\" target=\"_blank\">Bokeh</a> package. Beyond plotting SST field and anomalies we will take advantage of the recent <a href=\"https://github.com/CoLAB-ATLANTIC/JUNO\" target=\"_blank\">JUNO</a> Python package for calculating fronts using the <a href=\"https://journals.ametsoc.org/view/journals/atot/9/1/1520-0426_1992_009_0067_edafsi_2_0_co_2.xml\n",
    "\" target=\"_blank\">Cayula and Cornillion (1992)</a> algorithm.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='TOC_TOP'></a>Contents\n",
    "\n",
    "</div>\n",
    "    \n",
    " 1. [Preparating to run this script](#section1)\n",
    " 1. [Downloading data](#section2)\n",
    " 1. [Spatial plotting](#section3)\n",
    " 1. [Calculating and adding fronts](#section4)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing all of the libraries that we need to run this notebook. If you have built your python using the environment file provided in this repository, then you should have everything you need. For more information on building environment, please see the repository **<a href=\"../README.md\" target=\"_blank\">README</a>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os                                             # a library that allows us access to basic operating system commands\n",
    "import datetime                                       # a library that allows us to work with dates and times\n",
    "import numpy as np                                    # a library that lets us work with arrays; we import this with a new name \"np\"\n",
    "import glob                                           # a package that helps with file searching\n",
    "import xarray as xr                                   # a powerful library that helps us work efficiently with multi-dimensional arrays\n",
    "import IPython                                        # a library that helps us display video and HTML content\n",
    "import json                                           # a library that helps us make JSON format files\n",
    "from pydap.client import open_url                     # a library that helps us access OpenDAP data\n",
    "from pydap.cas.get_cookies import setup_session       # a library that helps us access OpenDAP data\n",
    "import bokeh.plotting as bk                           # a library that supports plotting\n",
    "from bokeh.palettes import *                          # as above\n",
    "from bokeh.models import ColorBar, LinearColorMapper  # as above\n",
    "from bokeh.layouts import gridplot                    # as above\n",
    "import warnings                                       # a library the helps us to manage warnings\n",
    "\n",
    "# turn off warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many Python packages that support ploting, such as Matplotlib and SeaBorn, but Bokeh, the package we will use, is particularly good at quickly plotting spatial data in an interactive way. Below we will initialise our Bokeh plot for notebook use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ed0ec6de-fc39-4dc6-809d-684070fec2cd\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"ed0ec6de-fc39-4dc6-809d-684070fec2cd\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.1.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"ed0ec6de-fc39-4dc6-809d-684070fec2cd\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"ed0ec6de-fc39-4dc6-809d-684070fec2cd\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.1.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ed0ec6de-fc39-4dc6-809d-684070fec2cd\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initiate \"bokeh\" plot\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to import the package we will use for front analysis. While all the other packages we use have been added to our environvment using the conda package manager, we will retrieve the JUNO front package directly from its GitHub repository. The cell below will do this for us, if we don't have the package installed already. You should see a file called \"JUNO\" in the navigation bar on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(os.getcwd(),\"JUNO\")):\n",
    "    print(\"Package exists\")\n",
    "else:\n",
    "    print(\"Fetching package\")\n",
    "    !git clone https://github.com/CoLAB-ATLANTIC/JUNO.git\n",
    "\n",
    "from JUNO.src import CayulaCornillon_xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a download directory to store the SST products we will download for use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a download directory for our SLSTR products\n",
    "download_dir = os.path.join(os.getcwd(), \"products\")\n",
    "os.makedirs(download_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "## <a id='section0'></a>0. Defining functions\n",
    "[Back to top](#TOC-TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will define a few functions that will help us manage data access. We make functions for code that we reuse. These cells are hidden by default, but you don't need to worry much about what they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_cmems_opendap(dataset, authentication_url, times, region, output_template, credentials_file):\n",
    "    '''\n",
    "    This function prodives an OpenDAP connection to the CMEMS Data Store\n",
    "    '''\n",
    "    data_store = copernicus_datastore(authentication_url, dataset, credentials_file)\n",
    "    DS = xr.open_dataset(data_store)\n",
    "   \n",
    "    files_list = []\n",
    "    for time in times:\n",
    "        print(f\"Extracting data for {str(time)}\")\n",
    "        subset = DS.analysed_sst.sel(lon=slice(region[0], region[2]), lat=slice(region[1], region[3])).sel(time=time)\n",
    "        output_file = os.path.join(download_dir, f\"{output_template}{time.split('T')[0]}.nc\")\n",
    "        files_list.append(output_file)\n",
    "        subset.to_netcdf(output_file)\n",
    "    \n",
    "    DS.close()\n",
    "    data_store.close()\n",
    "\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def copernicus_datastore(authentication_url, dataset, credentials_file):\n",
    "    '''\n",
    "    This function authenticates the CMEMS Data Store\n",
    "    '''\n",
    "    username, password = read_credentials_file(authentication_url, credentials_file, Oauth2=False)\n",
    "    session = setup_session('https://cmems-cas.cls.fr/cas/login', username, password)\n",
    "    session.cookies.set(\"CASTGC\", session.cookies.get_dict()['CASTGC'])    \n",
    "    data_store = xr.backends.PydapDataStore(open_url(f'{authentication_url}/thredds/dodsC/{dataset}',\n",
    "                                                     session=session, user_charset='utf-8'))\n",
    "    return data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_credentials_file(authentication_url, credentials_file=os.path.join(os.path.expanduser(\"~\"),'.eumdac_credentials'), Oauth2=True):\n",
    "    '''\n",
    "    This function reads authentication details from the CMEMS or EUMDAC credantials files that are stored locally. \n",
    "    By default these are stored in the home directory as the .cmems_opendap and .eumdac_credentials files with the following structures:\n",
    "    \n",
    "    .eumdac_credentials:\n",
    "    {\n",
    "    \"consumer_key\": \"<YOUR CONSUMER KEY>\",\n",
    "    \"consumer_secret\": \"<YOUR CONSUMER SECRET>\"\n",
    "    }\n",
    "\n",
    "    .cmems_opendap:\n",
    "    {\n",
    "    \"https://my.cmems-du.eu\": [\"<YOUR USERNAME>\", \"<YOUR PASSWORD>\"],\n",
    "    \"https://nrt.cmems-du.eu\": [\"<YOUR USERNAME>\", \"<YOUR PASSWORD>\"]\n",
    "    }\n",
    "\n",
    "    Note that the files should have no extension (e.g. do not add .txt). You should replace the contents of the <> with the relevant\n",
    "    details you obtain from EUMETSAT or CMEMS\n",
    "    '''\n",
    "    with open(credentials_file) as json_file:\n",
    "        credentials = json.load(json_file)\n",
    "        if Oauth2:\n",
    "            key, secret = credentials['consumer_key'], credentials['consumer_secret']\n",
    "            return key, secret\n",
    "        else:\n",
    "            username, password = credentials[authentication_url]\n",
    "            return username, password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section1'></a>1. Preparing to run this script\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to access the data needed to run the plotting routines in this script, you will need to sign up for a Copernicus Marine Service (CMEMS) account and create a file in the home directory of this system that contains your CMEMS username and password. This video will show you how to set this file up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://photos.smugmug.com/CMTS/Training-videos/i-GxkTMsV/0/079bbf8e/1280/Copernicus_marine_introduction-1280.mp4\" controls  width=\"700\"  height=\"450\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Video(url=\"https://photos.smugmug.com/CMTS/Training-videos/i-GxkTMsV/0/079bbf8e/1280/Copernicus_marine_introduction-1280.mp4\", width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section2'></a>2. Downloading data\n",
    "[Back to top](#TOC_TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to download data from the the Copernicus Marine Service data store. When set to `True`, this notebook will download new data. When set to `False` we will work with the data we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data downloading options\n",
    "download_cmems_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us pass the script the path to our recently created credentials file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials file\n",
    "cmems_credentials_file = os.path.join(os.path.expanduser(\"~\"),'.cmems_opendap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to download two sets of data. The fist is the **OSTIA \"multi-year\" daily level-4 reprocessed foundation temperature** data set, which we will use to create a climatology. The second is the analogous **OSTIA \"near real-time\" daily level-4 foundation temperature** data set, which we will use to view recent daily SST fields and anomalies. You can find more information on these data sets at the top of this script.\n",
    "\n",
    "The cell below sets the parameters we need to retrieve and these data sets...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multi-year reprocessing\n",
    "authentication_url_my = 'https://my.cmems-du.eu'\n",
    "dataset_my = 'METOFFICE-GLO-SST-L4-REP-OBS-SST'\n",
    "format_my = 'CLIM_OSTIA_SST_'\n",
    "\n",
    "# Near real-time\n",
    "authentication_url_nrt = 'https://nrt.cmems-du.eu'\n",
    "dataset_nrt = 'METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2'\n",
    "format_nrt = 'OPER_OSTIA_SST_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and these parameters define the spatial and temporal bounds of our data selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROI\n",
    "acquisition_ROI = [65.0, 0.0, 90.0, 25.0] # W, S, E, N\n",
    "\n",
    "# climatology\n",
    "start_year = 2000\n",
    "end_year = 2019\n",
    "month = 1\n",
    "day = 22\n",
    "clim_times = [f\"{ii}-{str(month).zfill(2)}-{str(day).zfill(2)}T12:00:00.000000000\" for ii in range(start_year, end_year+1)]\n",
    "\n",
    "# operational\n",
    "nrt_year = 2023\n",
    "nrt_time = [f\"{nrt_year}-{str(month).zfill(2)}-{str(day).zfill(2)}T12:00:00.000000000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets fetch our data! By default, this should appear in the \"products\" directory in the file manager on the left. We collect the paths to these products in a list, for later access. If we already had data downloaded we collect the existing products in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 2 column 39 (char 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m SST_files:\n\u001b[1;32m      4\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(f)\n\u001b[0;32m----> 6\u001b[0m     clim_files \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_cmems_opendap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthentication_url_my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclim_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macquisition_ROI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmems_credentials_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     SST_file \u001b[38;5;241m=\u001b[39m fetch_cmems_opendap(dataset_nrt, authentication_url_nrt, nrt_time, acquisition_ROI, format_nrt, cmems_credentials_file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mfetch_cmems_opendap\u001b[0;34m(dataset, authentication_url, times, region, output_template, credentials_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_cmems_opendap\u001b[39m(dataset, authentication_url, times, region, output_template, credentials_file):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function prodives an OpenDAP connection to the CMEMS Data Store\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     data_store \u001b[38;5;241m=\u001b[39m \u001b[43mcopernicus_datastore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthentication_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     DS \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(data_store)\n\u001b[1;32m      8\u001b[0m     files_list \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mcopernicus_datastore\u001b[0;34m(authentication_url, dataset, credentials_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopernicus_datastore\u001b[39m(authentication_url, dataset, credentials_file):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function authenticates the CMEMS Data Store\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     username, password \u001b[38;5;241m=\u001b[39m \u001b[43mread_credentials_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthentication_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOauth2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     session \u001b[38;5;241m=\u001b[39m setup_session(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cmems-cas.cls.fr/cas/login\u001b[39m\u001b[38;5;124m'\u001b[39m, username, password)\n\u001b[1;32m      7\u001b[0m     session\u001b[38;5;241m.\u001b[39mcookies\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCASTGC\u001b[39m\u001b[38;5;124m\"\u001b[39m, session\u001b[38;5;241m.\u001b[39mcookies\u001b[38;5;241m.\u001b[39mget_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCASTGC\u001b[39m\u001b[38;5;124m'\u001b[39m])    \n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mread_credentials_file\u001b[0;34m(authentication_url, credentials_file, Oauth2)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis function reads authentication details from the CMEMS or EUMDAC credantials files that are stored locally. \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mBy default these are stored in the home directory as the .cmems_opendap and .eumdac_credentials files with the following structures:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mdetails you obtain from EUMETSAT or CMEMS\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(credentials_file) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m---> 22\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Oauth2:\n\u001b[1;32m     24\u001b[0m         key, secret \u001b[38;5;241m=\u001b[39m credentials[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsumer_key\u001b[39m\u001b[38;5;124m'\u001b[39m], credentials[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsumer_secret\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_GHRSST24/lib/python3.8/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_GHRSST24/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_GHRSST24/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_GHRSST24/lib/python3.8/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 2 column 39 (char 40)"
     ]
    }
   ],
   "source": [
    "if download_cmems_data:\n",
    "    SST_files = glob.glob(os.path.join(download_dir, \"*\"))\n",
    "    for f in SST_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    clim_files = fetch_cmems_opendap(dataset_my, authentication_url_my, clim_times, acquisition_ROI, format_my, cmems_credentials_file)\n",
    "    SST_file = fetch_cmems_opendap(dataset_nrt, authentication_url_nrt, nrt_time, acquisition_ROI, format_nrt, cmems_credentials_file)\n",
    "else:\n",
    "    clim_files = glob.glob(os.path.join(download_dir, format_my + \"*\"))\n",
    "    SST_file = glob.glob(os.path.join(download_dir, format_nrt + \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to work with the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 3. <a id='section3'></a>Spatial plotting\n",
    "[Back to top](#TOC_TOP)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets plot what we have! Step one is to open our data files. In most of cases, these will be stored as netCDF (though on cloud platforms could be stored as zarr or cloud-optimised GeoTIFF). The `xarray` package is a powerful way to work with netCDF, and we will use this to open all of our files at once as a \"Dataset\". We will create two Datasets, one for our \"multi-year\" data and one for our \"near real-time\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_clim_data = xr.open_mfdataset(clim_files, combine='nested', concat_dim=\"time\")\n",
    "SST_data = xr.open_mfdataset(SST_file, combine='nested', concat_dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have these Datasets, we can manipulate them to calculate a daily climatology (taking a temporal mean) and an anomaly (by subtracting the climatology from our near real-time product)\n",
    "\n",
    "*Note: in the strictest sense, comparing reprocessed data with near real-time data is problematic as the latter has lower quality. However, practically, near real-time data offers the only opportunity for analysing recent events.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_clim = np.array(SST_clim_data[\"analysed_sst\"].mean(dim='time'))\n",
    "SST_oper = np.array(np.squeeze(SST_data[\"analysed_sst\"]))\n",
    "SST_anomaly = SST_oper - SST_clim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also read in some coordinate data to help with our plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonmin, lonmax = np.nanmin(SST_data[\"lon\"]), np.nanmax(SST_data[\"lon\"])\n",
    "latmin, latmax = np.nanmin(SST_data[\"lat\"]), np.nanmax(SST_data[\"lat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make our first plot, which will show our near real-time foundation SST for our chosen date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(interp_palette(RdYlBu11, 256),\n",
    "                                 low=np.nanmin(SST_oper),\n",
    "                                 high=np.nanmax(SST_oper))\n",
    "# plot\n",
    "f1 = bk.figure(width=1000, height=720, x_range=[lonmin, lonmax], y_range=[latmin, latmax])\n",
    "f1.image(image=[SST_oper], x=[lonmin], y=[latmin], dw=[lonmax-lonmin], dh=[latmax-latmin], color_mapper=color_mapper)\n",
    "\n",
    "clabel = f\"OSTIA level-4 foundation temperature [K] ({nrt_year}) for {day}.{month}\"\n",
    "color_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, location=(0,0), title=clabel)\n",
    "f1.add_layout(color_bar, 'right')\n",
    "\n",
    "bk.show(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using exactly the same approach, we can also plot our daily climatology for this time and region. However, this time, we are going to link the plots so that we can pan and zoom both the climatology and anomaly at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(interp_palette(RdYlBu11, 256),\n",
    "                                 low=np.nanmin(SST_clim),\n",
    "                                 high=np.nanmax(SST_clim))\n",
    "# plot\n",
    "f2 = bk.figure(width=550, height=400, x_range=[lonmin, lonmax], y_range=[latmin, latmax])\n",
    "f2.image(image=[SST_clim], x=[lonmin], y=[latmin], dw=[lonmax-lonmin], dh=[latmax-latmin], color_mapper=color_mapper)\n",
    "\n",
    "clabel = f\"OSTIA SST daily climatology [K] for {day}.{month} ({start_year} to {end_year})\"\n",
    "\n",
    "color_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, location=(0,0), title=clabel)\n",
    "f2.add_layout(color_bar, 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the anomaly between our near real-time product and the climatology.\n",
    "\n",
    "*Note: here we set the limits of the plot \"symetrically\" with regard to the data, to make sure that white sits at the zero anomaly. Palettes like this one, with white in the centre, should only ever be used for anomalies, and white should always sit at zero!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = LinearColorMapper(interp_palette(RdBu11, 256),\n",
    "                                 low=np.nanmax(np.abs(SST_anomaly))*-1,\n",
    "                                 high=np.nanmax(np.abs(SST_anomaly)))\n",
    "# plot\n",
    "f3 = bk.figure(width=550, height=400, x_range=f2.x_range, y_range=f2.y_range)\n",
    "f3.image(image=[SST_anomaly], x=[lonmin], y=[latmin], dw=[lonmax-lonmin], dh=[latmax-latmin], color_mapper=color_mapper)\n",
    "\n",
    "clabel = f\"OSTIA SST anomaly [K] ({nrt_year}) for {day}.{month} vs climatology\"\n",
    "\n",
    "color_bar = ColorBar(color_mapper=color_mapper, label_standoff=12, border_line_color=None, location=(0,0), title=clabel)\n",
    "f3.add_layout(color_bar, 'right')\n",
    "\n",
    "bk.show(gridplot([[f2, f3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our plot, that, despite some coastal cooling, our chosen date is, on the whole, substantially warmer in 2023 than the corresponding climatology. This is a single day snapshot, and we are comparing reprocessed with near-real time data, so we have to be careful with the interpretation, but it paints a picture that is fairly representative of the ocean warming signals currently being observed globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## 4. <a id='section4'></a>Calculating and adding fronts\n",
    "[Back to top](#TOC_TOP)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean SST values and anomalies are essential fields for monitoring temporal changes, but sometimes it is spatial variability that we are most interested in (e.g., if we are interested in the effects of physical system on fish populations). One of the best ways to extract this information is to analyse SST fields for the presence of fronts; regions of strong spatial gradients.\n",
    "\n",
    "Above, we imported the <a href=\"https://github.com/CoLAB-ATLANTIC/JUNO\" target=\"_blank\">JUNO</a> package, which allows us access to a variety of front detection methods. Here, we are choosing to apply the <a href=\"https://journals.ametsoc.org/view/journals/atot/9/1/1520-0426_1992_009_0067_edafsi_2_0_co_2.xml\n",
    "\" target=\"_blank\">Cayula and Cornillion (1992)</a> algorithm (CCA) to our near-real time data. CCA is probably the most widely used algorithm to detect ocean fronts, but the package also offers options for the Canny (1986) and Belkin and O'Reilly (2009) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# for interest, the %%capture statement above suppresses all output from this cell, as the function makes a plot that we don't really want!\n",
    "fronts_x, fronts_y = CayulaCornillon_xarray.CCA_SIED(SST_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply plot our fronts over our existing SST field (saved as plot `f1` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.scatter(fronts_x, fronts_y, color=\"black\", alpha=0.5, size=1)\n",
    "bk.show(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the fronts correspond regions that we could roughly identify by eye in this single image, but on a statistically quantitative basis. Additionally, the power of this method is that it can be used to determine the persistence of fronts over time, informing further questions about possible fish habitats.\n",
    "\n",
    "That concludes out quick notebook, but if you have more questions about plotting SST products, then please contact us at the email address linked below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"https://github.com/wekeo/GHRSST24\" target=\"_blank\">View on GitHub</a> | <a href=\"https://training.eumetsat.int/\" target=\"_blank\">EUMETSAT Training</a> | <a href=mailto:ops@eumetsat.int target=\"_blank\">Contact helpdesk for support </a> | <a href=mailto:Copernicus.training@eumetsat.int target=\"_blank\">Contact our training team to collaborate on and reuse this material</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "author": "Ben Loveday, Hayley Evers-King",
  "description": "This Jupyter Notebook shows how to access Sentinel-3 Sea and Land Surface Temperature Radiometer (SLSTR) data via the EUMETSAT Data Store.",
  "image": "../img/thumbs/1_1a_SLSTR_data_access_Data_Store_thumb.png",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "services": {
   "eumetsat": {
    "binder": {
     "link": "https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.eumetsat.int%2Feumetlab%2Foceans%2Focean-training%2Fsensors%2Flearn-slstr/HEAD?urlpath=%2Ftree%2F1_SLSTR_introductory%2F1_1a_SLSTR_data_access_Data_Store.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "git": {
     "link": "https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/sensors/learn-slstr/-/blob/main/1_SLSTR_introductory/1_1a_SLSTR_data_access_Data_Store.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   },
   "wekeo": {
    "git": {
     "link": "",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "url": {
     "link": "",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   }
  },
  "tags": {
   "domain": "Marine",
   "platform": [
    "Sentinel-3"
   ],
   "sensor": "SLSTR",
   "service": "EUMETSAT",
   "subtheme": [
    "Ocean dynamics"
   ],
   "tags": [
    "Top-of-atmosphere radiance",
    "Top-of-atmosphere brightness temperature",
    "Sea ice surface temperature",
    "Sea surface temperature"
   ]
  },
  "title": "Accessing Sentinel-3 SLSTR data via the EUMETSAT Data Store"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
